{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRH0NbrFXOPr",
        "outputId": "f4aec5d4-e062-45c5-b68a-fdc6a82f0832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== a) parallelize() ===\n",
            "Parallelized RDD: [10, 20, 30, 40, 50]\n",
            "\n",
            "=== b) Read Text File & CSV ===\n",
            "Text File RDD: ['This is line one.', 'This is line two.', 'This is line three.']\n",
            "CSV File RDD (without header): ['Alice,21', 'Bob,22']\n",
            "\n",
            "=== c) Create RDD ===\n",
            "Created RDD: ['CS', 'Maths', 'Physics']\n",
            "\n",
            "=== d) RDD Actions ===\n",
            "Count: 5\n",
            "First: 1\n",
            "Take 3: [1, 2, 3]\n",
            "Sum using reduce: 15\n",
            "\n",
            "=== e) Pair Functions ===\n",
            "reduceByKey: [('CS', 30), ('Maths', 15), ('Physics', 10)]\n",
            "groupByKey: [('CS', [10, 20]), ('Maths', [15]), ('Physics', [10])]\n",
            "mapValues (values * 2): [('CS', 20), ('Maths', 30), ('CS', 40), ('Physics', 20)]\n",
            "Keys: ['CS', 'Maths', 'CS', 'Physics']\n",
            "Values: [10, 15, 20, 10]\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark RDD Examples\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# a) PARALLELIZE\n",
        "print(\"\\n=== a) parallelize() ===\")\n",
        "data = [10, 20, 30, 40, 50]\n",
        "rdd_parallel = sc.parallelize(data)\n",
        "print(\"Parallelized RDD:\", rdd_parallel.collect())\n",
        "\n",
        "# b) READ TEXT FILE & CSV\n",
        "print(\"\\n=== b) Read Text File & CSV ===\")\n",
        "\n",
        "# Text File (Assuming file.txt exists)\n",
        "# Example content: line1\\nline2\\nline3\n",
        "rdd_text = sc.textFile(\"sample_text.txt\")\n",
        "print(\"Text File RDD:\", rdd_text.take(3))  # Show first 3 lines\n",
        "\n",
        "# CSV File (Assuming file.csv exists)\n",
        "# Example content:\n",
        "# name,age\n",
        "# Alice,21\n",
        "# Bob,22\n",
        "rdd_csv = sc.textFile(\"sample_csv.csv\")\n",
        "header = rdd_csv.first()\n",
        "rdd_csv_data = rdd_csv.filter(lambda row: row != header)\n",
        "print(\"CSV File RDD (without header):\", rdd_csv_data.take(2))\n",
        "\n",
        "# c) CREATE RDD\n",
        "print(\"\\n=== c) Create RDD ===\")\n",
        "rdd_created = sc.parallelize([\"CS\", \"Maths\", \"Physics\"])\n",
        "print(\"Created RDD:\", rdd_created.collect())\n",
        "\n",
        "# d) ACTIONS\n",
        "print(\"\\n=== d) RDD Actions ===\")\n",
        "numbers = sc.parallelize([1, 2, 3, 4, 5])\n",
        "print(\"Count:\", numbers.count())\n",
        "print(\"First:\", numbers.first())\n",
        "print(\"Take 3:\", numbers.take(3))\n",
        "print(\"Sum using reduce:\", numbers.reduce(lambda x, y: x + y))\n",
        "\n",
        "# e) PAIR FUNCTIONS\n",
        "print(\"\\n=== e) Pair Functions ===\")\n",
        "pair_rdd = sc.parallelize([(\"CS\", 10), (\"Maths\", 15), (\"CS\", 20), (\"Physics\", 10)])\n",
        "\n",
        "# reduceByKey\n",
        "reduced = pair_rdd.reduceByKey(lambda a, b: a + b)\n",
        "print(\"reduceByKey:\", reduced.collect())\n",
        "\n",
        "# groupByKey\n",
        "grouped = pair_rdd.groupByKey().mapValues(list)\n",
        "print(\"groupByKey:\", grouped.collect())\n",
        "\n",
        "# mapValues\n",
        "mapped = pair_rdd.mapValues(lambda x: x * 2)\n",
        "print(\"mapValues (values * 2):\", mapped.collect())\n",
        "\n",
        "# keys and values\n",
        "print(\"Keys:\", pair_rdd.keys().collect())\n",
        "print(\"Values:\", pair_rdd.values().collect())\n",
        "\n",
        "# Stop Spark Session\n",
        "spark.stop()\n"
      ]
    }
  ]
}