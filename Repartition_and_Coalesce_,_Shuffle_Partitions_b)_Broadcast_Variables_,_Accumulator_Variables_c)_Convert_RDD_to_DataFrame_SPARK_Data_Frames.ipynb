{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU0dJ9MpYkVw",
        "outputId": "7d59bd8c-57d8-40c6-d8b4-84e09e0fca31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== a) Repartition & Coalesce ===\n",
            "Initial Partitions: 4\n",
            "After Repartition: 6\n",
            "After Coalesce: 2\n",
            "Default Shuffle Partitions: 200\n",
            "Updated Shuffle Partitions: 10\n",
            "\n",
            "=== b) Broadcast & Accumulator ===\n",
            "With Gender Info: [('Alice', 21, 'F'), ('Bob', 22, 'M'), ('Charlie', 23, 'M'), ('Alice', 21, 'F')]\n",
            "Accumulator count: 4\n",
            "\n",
            "=== c) RDD to DataFrame ===\n",
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|  Alice| 21|\n",
            "|    Bob| 22|\n",
            "|Charlie| 23|\n",
            "|  Alice| 21|\n",
            "+-------+---+\n",
            "\n",
            "\n",
            "=== createDataFrame() ===\n",
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|David| 24|\n",
            "|  Eva| 25|\n",
            "+-----+---+\n",
            "\n",
            "\n",
            "=== where() and filter() ===\n",
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|    Bob| 22|\n",
            "|Charlie| 23|\n",
            "+-------+---+\n",
            "\n",
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|    Bob| 22|\n",
            "|Charlie| 23|\n",
            "+-------+---+\n",
            "\n",
            "\n",
            "=== withColumn() ===\n",
            "+-------+---+----------+\n",
            "|   name|age|age_plus_1|\n",
            "+-------+---+----------+\n",
            "|  Alice| 21|        22|\n",
            "|    Bob| 22|        23|\n",
            "|Charlie| 23|        24|\n",
            "|  Alice| 21|        22|\n",
            "+-------+---+----------+\n",
            "\n",
            "+-------+---+-------------+\n",
            "|   name|age|age_next_year|\n",
            "+-------+---+-------------+\n",
            "|  Alice| 21|           22|\n",
            "|    Bob| 22|           23|\n",
            "|Charlie| 23|           24|\n",
            "|  Alice| 21|           22|\n",
            "+-------+---+-------------+\n",
            "\n",
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|  Alice| 21|\n",
            "|    Bob| 22|\n",
            "|Charlie| 23|\n",
            "|  Alice| 21|\n",
            "+-------+---+\n",
            "\n",
            "\n",
            "=== distinct() ===\n",
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|  Alice| 21|\n",
            "|    Bob| 22|\n",
            "|Charlie| 23|\n",
            "+-------+---+\n",
            "\n",
            "\n",
            "=== groupBy() ===\n",
            "+-------+-----+\n",
            "|   name|count|\n",
            "+-------+-----+\n",
            "|  Alice|    2|\n",
            "|    Bob|    1|\n",
            "|Charlie|    1|\n",
            "+-------+-----+\n",
            "\n",
            "\n",
            "=== join() ===\n",
            "+----+---+---+\n",
            "|name|age|age|\n",
            "+----+---+---+\n",
            "+----+---+---+\n",
            "\n",
            "\n",
            "=== map() vs mapPartitions() ===\n",
            "map: [('Alice', 31), ('Bob', 32), ('Charlie', 33), ('Alice', 31)]\n",
            "mapPartitions: [('Alice', 42), ('Bob', 44), ('Charlie', 46), ('Alice', 42)]\n",
            "\n",
            "=== foreach() vs foreachPartition() ===\n",
            "\n",
            "=== pivot() ===\n",
            "+-------+----+----+----+\n",
            "|   name|  21|  22|  23|\n",
            "+-------+----+----+----+\n",
            "|    Bob|NULL|   1|NULL|\n",
            "|  Alice|   2|NULL|NULL|\n",
            "|Charlie|NULL|NULL|   1|\n",
            "+-------+----+----+----+\n",
            "\n",
            "\n",
            "=== union() ===\n",
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|  Alice| 21|\n",
            "|    Bob| 22|\n",
            "|Charlie| 23|\n",
            "|  Alice| 21|\n",
            "|  David| 24|\n",
            "|    Eva| 25|\n",
            "+-------+---+\n",
            "\n",
            "\n",
            "=== collect() ===\n",
            "Collect result: [Row(name='Alice', age=21), Row(name='Bob', age=22), Row(name='Charlie', age=23), Row(name='Alice', age=21)]\n",
            "\n",
            "=== cache() and persist() ===\n",
            "\n",
            "=== udf() ===\n",
            "+-------+---+----------+\n",
            "|   name|age|upper_name|\n",
            "+-------+---+----------+\n",
            "|  Alice| 21|     ALICE|\n",
            "|    Bob| 22|       BOB|\n",
            "|Charlie| 23|   CHARLIE|\n",
            "|  Alice| 21|     ALICE|\n",
            "+-------+---+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Advanced Spark RDD and DataFrame Examples\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Sample data\n",
        "data = [(\"Alice\", 21), (\"Bob\", 22), (\"Charlie\", 23), (\"Alice\", 21)]\n",
        "rdd = sc.parallelize(data, 4)\n",
        "\n",
        "# ============ RDD PART ============\n",
        "\n",
        "# a) Repartition and Coalesce, Shuffle Partitions\n",
        "print(\"\\n=== a) Repartition & Coalesce ===\")\n",
        "print(\"Initial Partitions:\", rdd.getNumPartitions())\n",
        "rdd_repartitioned = rdd.repartition(6)\n",
        "print(\"After Repartition:\", rdd_repartitioned.getNumPartitions())\n",
        "\n",
        "rdd_coalesced = rdd_repartitioned.coalesce(2)\n",
        "print(\"After Coalesce:\", rdd_coalesced.getNumPartitions())\n",
        "\n",
        "# Shuffle Partitions (default is 200, can be changed)\n",
        "print(\"Default Shuffle Partitions:\", spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")\n",
        "print(\"Updated Shuffle Partitions:\", spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
        "\n",
        "# b) Broadcast Variables and Accumulator Variables\n",
        "print(\"\\n=== b) Broadcast & Accumulator ===\")\n",
        "broadcast_var = sc.broadcast({\"Alice\": \"F\", \"Bob\": \"M\", \"Charlie\": \"M\"})\n",
        "acc = sc.accumulator(0)\n",
        "\n",
        "def gender_mapper(name_age):\n",
        "    global acc\n",
        "    name, age = name_age\n",
        "    if name in broadcast_var.value:\n",
        "        acc += 1\n",
        "    return (name, age, broadcast_var.value.get(name, \"Unknown\"))\n",
        "\n",
        "rdd_with_gender = rdd.map(gender_mapper)\n",
        "print(\"With Gender Info:\", rdd_with_gender.collect())\n",
        "print(\"Accumulator count:\", acc.value)\n",
        "\n",
        "# c) Convert RDD to DataFrame\n",
        "print(\"\\n=== c) RDD to DataFrame ===\")\n",
        "df = rdd.toDF([\"name\", \"age\"])\n",
        "df.show()\n",
        "\n",
        "# ============ DATAFRAME PART ============\n",
        "\n",
        "# createDataFrame()\n",
        "print(\"\\n=== createDataFrame() ===\")\n",
        "df2 = spark.createDataFrame([(\"David\", 24), (\"Eva\", 25)], [\"name\", \"age\"])\n",
        "df2.show()\n",
        "\n",
        "# where() and filter()\n",
        "print(\"\\n=== where() and filter() ===\")\n",
        "df.filter(col(\"age\") > 21).show()\n",
        "df.where(\"age > 21\").show()\n",
        "\n",
        "# withColumn()\n",
        "print(\"\\n=== withColumn() ===\")\n",
        "df = df.withColumn(\"age_plus_1\", col(\"age\") + 1)\n",
        "df.show()\n",
        "\n",
        "# withColumnRenamed()\n",
        "df = df.withColumnRenamed(\"age_plus_1\", \"age_next_year\")\n",
        "df.show()\n",
        "\n",
        "# drop()\n",
        "df = df.drop(\"age_next_year\")\n",
        "df.show()\n",
        "\n",
        "# distinct()\n",
        "print(\"\\n=== distinct() ===\")\n",
        "df.distinct().show()\n",
        "\n",
        "# groupBy()\n",
        "print(\"\\n=== groupBy() ===\")\n",
        "df.groupBy(\"name\").count().show()\n",
        "\n",
        "# join()\n",
        "print(\"\\n=== join() ===\")\n",
        "df_join = df.join(df2, on=\"name\", how=\"inner\")\n",
        "df_join.show()\n",
        "\n",
        "# map() vs mapPartitions()\n",
        "print(\"\\n=== map() vs mapPartitions() ===\")\n",
        "rdd_mapped = rdd.map(lambda x: (x[0], x[1] + 10))\n",
        "rdd_partitioned = rdd.mapPartitions(lambda iter: [(x[0], x[1] * 2) for x in iter])\n",
        "print(\"map:\", rdd_mapped.collect())\n",
        "print(\"mapPartitions:\", rdd_partitioned.collect())\n",
        "\n",
        "# foreach() vs foreachPartition()\n",
        "print(\"\\n=== foreach() vs foreachPartition() ===\")\n",
        "\n",
        "def print_each(record):\n",
        "    print(\"foreach:\", record)\n",
        "\n",
        "def print_partition(partition):\n",
        "    for record in partition:\n",
        "        print(\"foreachPartition:\", record)\n",
        "\n",
        "rdd.foreach(print_each)\n",
        "rdd.foreachPartition(print_partition)\n",
        "\n",
        "# pivot()\n",
        "print(\"\\n=== pivot() ===\")\n",
        "df_pivot = df.groupBy(\"name\").pivot(\"age\").count()\n",
        "df_pivot.show()\n",
        "\n",
        "# union()\n",
        "print(\"\\n=== union() ===\")\n",
        "df_union = df.union(df2)\n",
        "df_union.show()\n",
        "\n",
        "# collect()\n",
        "print(\"\\n=== collect() ===\")\n",
        "print(\"Collect result:\", df.collect())\n",
        "\n",
        "# cache() and persist()\n",
        "print(\"\\n=== cache() and persist() ===\")\n",
        "df.cache()\n",
        "df.count()  # Triggers caching\n",
        "df.persist()\n",
        "df.count()  # Triggers persist\n",
        "\n",
        "# udf()\n",
        "print(\"\\n=== udf() ===\")\n",
        "def upper_name(name):\n",
        "    return name.upper()\n",
        "\n",
        "upper_udf = udf(upper_name, StringType())\n",
        "df = df.withColumn(\"upper_name\", upper_udf(col(\"name\")))\n",
        "df.show()\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n"
      ]
    }
  ]
}